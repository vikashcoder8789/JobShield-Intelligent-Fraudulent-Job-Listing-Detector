# -*- coding: utf-8 -*-
"""fraudulent job_project .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17bMnHQnE9pG6ByGgojs3s_DxTCACJ01Z
"""

import pandas as pd
from sklearn.model_selection import train_test_split, RandomizedSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
import re
import time
from sklearn.pipeline import Pipeline

# Load data
train_df = pd.read_csv("train_data.csv")

test_df = pd.read_csv("train_data.csv")

# Preprocess and clean text
def clean_text(text):
    text = text.lower()
    text = re.sub(r'<[^>]+>', ' ', text)  # remove HTML tags
    text = re.sub(r'[^a-z0-9\s]', ' ', text)  # keep alphanumeric
    text = re.sub(r'\s+', ' ', text).strip()
    return text

text_cols = ['title', 'company_profile', 'description', 'requirements', 'benefits']
for df in [train_df, test_df]:
    for col in text_cols:
        df[col] = df[col].fillna('').apply(clean_text)
    df['combined_text'] = df[text_cols].agg(' '.join, axis=1)

# Features and labels
X = train_df['combined_text']
y = train_df['fraudulent']

# Split for evaluation
X_train_text, X_val_text, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

# Vectorizer
vectorizer = TfidfVectorizer(
    max_features=5000,
    ngram_range=(1,2),
    stop_words='english',
    min_df=5,
    sublinear_tf=True
)
X_train_vec = vectorizer.fit_transform(X_train_text)
X_val_vec = vectorizer.transform(X_val_text)

# Apply SMOTE
smote = SMOTE(random_state=42, k_neighbors=3)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vec, y_train)

# Train RandomForest
print("Training RandomForest...")
start = time.time()
model = RandomForestClassifier(class_weight='balanced', random_state=42)
model.fit(X_train_resampled, y_train_resampled)
duration = time.time() - start
val_preds = model.predict(X_val_vec)
f1 = f1_score(y_val, val_preds)
acc = accuracy_score(y_val, val_preds)
print(f"RandomForest -> F1: {f1:.4f}, Accuracy: {acc:.4f}, Time: {duration:.2f} sec")

# Save pipeline
model_pipeline = Pipeline([
    ("tfidf", vectorizer),
    ("clf", model)
])
joblib.dump(model_pipeline, "fraud_model.pkl")

# Evaluate model
print("\nClassification Report:")
print(classification_report(y_val, val_preds))

# Confusion matrix
cm = confusion_matrix(y_val, val_preds)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Genuine', 'Fraud'], yticklabels=['Genuine', 'Fraud'])
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Validation Set Confusion Matrix')
plt.show()

# Predict on test set
test_df['combined_text'] = test_df['combined_text'].fillna('')
test_vec = vectorizer.transform(test_df['combined_text'])
test_probs = model.predict_proba(test_vec)[:, 1]
test_preds = model.predict(test_vec)
test_df['fraud_probability'] = test_probs
test_df['fraud_prediction'] = test_preds
test_df[['job_id', 'title', 'fraud_probability', 'fraud_prediction']].to_csv("test_predictions.csv", index=False)

print("\n‚úÖ Final predictions saved to test_predictions.csv")

# Example of adjusting threshold on validation set
from sklearn.metrics import classification_report

# Assuming you have model, X_val, y_val from previous steps
val_probs = model_pipeline.predict_proba(X_val_text)[:, 1]

# Experiment with a lower threshold, e.g., 0.3
new_threshold = 0.49
val_preds_tuned = (val_probs >= new_threshold).astype(int)

print(f"\nüìã Classification Report with Threshold {new_threshold}:\n")
print(classification_report(y_val, val_preds_tuned, target_names=['Genuine', 'Fraud']))

# You would repeat this for different thresholds (e.g., 0.2, 0.4)
# and observe how Recall for 'Fraud' and Precision for 'Fraud' change.
# Once you find a suitable threshold, apply it when making final predictions on the test set.
# test_preds_tuned = (test_probs >= chosen_threshold).astype(int)

cm=confusion_matrix(y_val,val_preds_tuned)
cm
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Genuine', 'Fraud'], yticklabels=['Genuine', 'Fraud'])



import smtplib
from email.mime.text import MIMEText

def send_email_alert(job_title, fraud_prob, recipient="you@example.com"):
    body = f"üö® ALERT: High-Risk Job Listing Detected!\n\nTitle: {job_title}\nFraud Probability: {fraud_prob:.2f}"
    msg = MIMEText(body)
    msg['Subject'] = "üö® Fraud Detection Alert"
    msg['From'] = "fraud.alert@example.com"
    msg['To'] = recipient

    # Use SMTP server like Gmail (for demo purpose)
    try:
        with smtplib.SMTP("smtp.gmail.com", 587) as server:
            server.starttls()
            server.login("your_email@gmail.com", "your_app_password")
            server.send_message(msg)
            print(f"‚úÖ Alert email sent to {recipient}")
    except Exception as e:
        print(f"‚ùå Email failed: {e}")

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib import rcParams

# Load the predictions
df = pd.read_csv("test_predictions.csv")  # Update path if needed

# Set global Seaborn style
sns.set_style("whitegrid")
rcParams['font.family'] = 'sans-serif'
rcParams['font.size'] = 11

# Create figure
fig, axes = plt.subplots(2, 2, figsize=(18, 12), facecolor='whitesmoke')
fig.suptitle("üõ°Ô∏è Job Listing Fraud Detection Dashboard", fontsize=22, fontweight='bold', color='darkblue', y=0.97)

# --- 1. Table of Results ---
axes[0, 0].axis('off')
table_data = df[['job_id', 'title', 'fraud_probability', 'fraud_prediction']].head(10)
table = axes[0, 0].table(cellText=table_data.values,
                         colLabels=table_data.columns,
                         loc='center',
                         cellLoc='center',
                         colColours=["#4472C4"]*4,
                         colWidths=[0.12, 0.6, 0.14, 0.14])
table.auto_set_font_size(False)
table.set_fontsize(9)
axes[0, 0].set_title("üìã Top 10 Job Listings", fontsize=16, fontweight='bold', pad=12)

# --- 2. Histogram of Fraud Probabilities ---
sns.histplot(df['fraud_probability'], bins=30, kde=True, ax=axes[0, 1], color="#4B9CD3", edgecolor='black')
axes[0, 1].set_title("üìà Histogram of Fraud Probabilities", fontsize=16, fontweight='bold')
axes[0, 1].set_xlabel("Fraud Probability")
axes[0, 1].set_ylabel("Number of Listings")

# --- 3. Pie Chart: Fake vs Real ---
fraud_counts = df['fraud_prediction'].value_counts()
labels = ['Real (0)', 'Fraud (1)']
colors = ['#8BC34A', '#E91E63']
axes[1, 0].pie(fraud_counts,
               labels=labels,
               autopct='%1.1f%%',
               colors=colors,
               startangle=140,
               textprops={'fontsize': 12})
axes[1, 0].set_title("üü† Distribution: Real vs Fraudulent Jobs", fontsize=16, fontweight='bold')

# --- 4. Top 10 Most Suspicious Listings ---
top_suspicious = df.sort_values("fraud_probability", ascending=False).head(10)
sns.barplot(y=top_suspicious['title'],
            x=top_suspicious['fraud_probability'],
            ax=axes[1, 1],
            palette=sns.color_palette("flare", 10))
axes[1, 1].set_title("üîç Top 10 Most Suspicious Job Listings", fontsize=16, fontweight='bold')
axes[1, 1].set_xlabel("Fraud Probability")
axes[1, 1].set_ylabel("Job Title")

plt.tight_layout(rect=[0, 0.03, 1, 0.95])
plt.show()



